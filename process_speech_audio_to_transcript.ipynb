{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "from google.cloud import speech\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from io import BufferedReader\n",
    "from pydub import AudioSegment\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples/snippets/quickstart.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google's Asynchronous Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud: load credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cloud.google.com/speech-to-text/docs/quickstart-client-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptrfile_to_credentials = r'.\\links\\my_credentials_location.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ptrfile_to_credentials,'r') as f:\n",
    "    myKeyFile_json = f.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google instructions talk about using ENV variables and installing SDK. Neither is ideal. ENV variables would have to be reset every time at the command prompt (because they clear upon exit). And SDK installation is overkill.\n",
    "\n",
    "If you've gone through the oauth2 client process and already obtained a developer credential token, you won't have to do either of those. You can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    !set GOOGLE_APPLICATION_CREDENTIALS=\"path_to_file_inside_these_quotes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/50445556/how-to-add-credentials-to-google-text-to-speech-api\n",
    "from google.oauth2 import service_account\n",
    "credentials = service_account.Credentials.from_service_account_file(myKeyFile_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = speech.SpeechClient(credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.speech_v1.SpeechClient object at 0x000002A212119AF0>\n"
     ]
    }
   ],
   "source": [
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credentials successfully delivered to SpeechClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous SR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asynchronous Speech Recognition must use Google Cloud Storage. Audio files up to 480 minutes can be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import wave\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brooklyn Bridge example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_uri = \"gs://cloud-samples-data/speech/brooklyn_bridge.raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: how old is the Brooklyn Bridge\n"
     ]
    }
   ],
   "source": [
    "# Example: GCS, Brooklyn Bridge\n",
    "# https://cloud.google.com/speech-to-text/docs/libraries#windows\n",
    "\n",
    "gcs_uri = \"gs://cloud-samples-data/speech/brooklyn_bridge.raw\"\n",
    "audio = speech.RecognitionAudio(uri=gcs_uri) \n",
    "config = speech.RecognitionConfig(\n",
    "    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "    sample_rate_hertz=16000,\n",
    "    language_code=\"en-US\",\n",
    ")\n",
    "\n",
    "# Detects speech in the audio file\n",
    "response = client.recognize(config=config, audio=audio)\n",
    "for result in response.results:\n",
    "    print(\"Transcript: {}\".format(result.alternatives[0].transcript))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_name = \"my_audio_recording.flac\"\n",
    "bucket_name = \"my-bucket-name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function only works if the input file is a WAV\n",
    "def frame_rate_channel(audio_file_name):\n",
    "    with wave.open(audio_file_name, \"rb\") as wave_file:\n",
    "        frame_rate = wave_file.getframerate()\n",
    "        channels = wave_file.getnchannels()\n",
    "        return frame_rate,channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(frame_rate_channel('audio4gcs/VoiceRecording - Copy.wav'))\n",
    "#(44100, 2)\n",
    "#print(frame_rate_channel('audio4gcs/RICKER AUDIO.wav'))\n",
    "#(22050, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n(framerate,numchannels) = frame_rate_channel(os.path.join('audio4gcs',audio_file_name))\\nprint((framerate,numchannels))\\n\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if WAV\n",
    "'''\n",
    "(framerate,numchannels) = frame_rate_channel(os.path.join('audio4gcs',audio_file_name))\n",
    "print((framerate,numchannels))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://my-bucket-name/my_audio_recording.flac\n"
     ]
    }
   ],
   "source": [
    "gcs_uri = r'gs://' + bucket_name + '/' + audio_file_name\n",
    "print(gcs_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long-Running Recognize:\n",
    "\n",
    "audio = speech.RecognitionAudio(uri=gcs_uri) \n",
    "\n",
    "diarization_config = speech.SpeakerDiarizationConfig(enable_speaker_diarization=True)\n",
    "\n",
    "\n",
    "config = speech.RecognitionConfig(\n",
    "    #encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "    #sample_rate_hertz=framerate,\n",
    "    language_code=\"en-US\",\n",
    "    diarization_config=diarization_config,\n",
    "    enable_automatic_punctuation=True,\n",
    "    #audio_channel_count=numchannels,   # defaults to one channel if omitted\n",
    "    #enable_separate_recognition_per_channel=True,\n",
    ")\n",
    "\n",
    "# Detects speech in the audio file\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "google.cloud.speech_v1.types.cloud_speech.SpeakerDiarizationConfig"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech.SpeakerDiarizationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uri: \"gs://my-bucket-name/my_audio_recording.flac\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Async SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation = client.long_running_recognize(config=config, audio=audio)\n",
    "# You might see an error here if task completion would exceed your quota\n",
    "# Quota can be configured within Google Developer account dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3631.5423810482025\n"
     ]
    }
   ],
   "source": [
    "tic=time.time()\n",
    "response = operation.result(timeout=30000)\n",
    "print(time.time()-tic)\n",
    "\n",
    "# Some example running times:\n",
    "# 10min 1-channel WAV took 120s\n",
    "# 1h WAV with 2 channels took 10m (2 channels)\n",
    "# M J-M Interview (4h) required 3631s (1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response     # output can be really long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "859"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the entire object in case we want to revisit later.\n",
    "\n",
    "words_obj = response.results[-1].alternatives[0].words\n",
    "google_output={}\n",
    "for field in ['start_time','end_time','word','speaker_tag']:\n",
    "    google_output[field]=[getattr((words_obj[j]),field) for j in range(len(words_obj))]\n",
    "\n",
    "audio_filenamebase = os.path.splitext(os.path.split(gcs_uri)[-1])[0]\n",
    "now_str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "output_filename = 'output_'+audio_filenamebase+now_str+'.p'\n",
    "pickle.dump(google_output, open(output_filename, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcript: OPTION 1: \n",
    "    # Use Speaker Diarization\n",
    "    # New line whenever speaker ID changes\n",
    "    \n",
    "result = response.results[-1] #Changed\n",
    "words_info = result.alternatives[0].words #Changed\n",
    "    \n",
    "tag=1 #Changed\n",
    "sentence=\"\" #Changed\n",
    "transcript=[]\n",
    "(minutes_start,seconds_start) = (0,0)\n",
    "\n",
    "for word_info in words_info: #Changed\n",
    "    if word_info.speaker_tag==tag: # continue current speaker\n",
    "        sentence=sentence+\" \"+word_info.word \n",
    "    else: # harvest current line, start new line to change speaker\n",
    "        (minutes_cur,seconds_cur) = divmod(word_info.start_time.seconds,60)\n",
    "        transcript.append(\" speaker %s: (%02d:%02d-%02d:%02d) %s\" %\\\n",
    "                              (tag,minutes_start,seconds_start,minutes_cur,seconds_cur,sentence))\n",
    "        tag=word_info.speaker_tag #Changed\n",
    "        sentence=\"\"+word_info.word #Changed\n",
    "        (minutes_start,seconds_start) = (minutes_cur,seconds_cur)\n",
    "        #timestamp = word_info.start_time.seconds + 1e-6*word_info.start_time.microseconds\n",
    "\n",
    "(minutes_cur,seconds_cur) = divmod(word_info.start_time.seconds,60)\n",
    "transcript.append(\" speaker %s: (%02d:%02d-%02d:%02d) %s\" %\\\n",
    "                              (tag,minutes_start,seconds_start,minutes_cur,seconds_cur,sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = audio_filenamebase+'_v1.txt'\n",
    "with open(outfile,'a') as f:\n",
    "    for s in transcript:\n",
    "        f.write(s+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30355"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcript: OPTION 2: \n",
    "    # Fixed-length lines.\n",
    "    # No speaker diarization\n",
    "    # New line every 15sec audio\n",
    "\n",
    "result = response.results[-1]\n",
    "words_info = result.alternatives[0].words\n",
    "    \n",
    "sentence=\"\" #Changed\n",
    "transcript=[]\n",
    "(m0,s0) = (0,0) # (minutes,seconds) start timeblock\n",
    "ts_final = words_info[-1].start_time.seconds\n",
    "timeblock_seconds = 15\n",
    "\n",
    "j=0\n",
    "for ts_start in range(0,1+ts_final,timeblock_seconds):\n",
    "    ts_end = ts_start + timeblock_seconds\n",
    "    (m1,s1) = divmod(ts_end,60) # (minutes,seconds) end timeblock\n",
    "    # get words in current timeblock:\n",
    "    while ((j<len(words_info)) and (words_info[j].start_time.seconds<ts_end)):\n",
    "        sentence=sentence+\" \"+words_info[j].word\n",
    "        j+=1\n",
    "    # harvest sentence, and increment to next timeblock:\n",
    "    transcript.append(\"(%02d:%02d-%02d:%02d) %s\" %\\\n",
    "                         (m0,s0,m1,s1,sentence))\n",
    "    sentence=\"\"\n",
    "    (m0,s0)=(m1,s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = audio_filenamebase+'_v2.txt'\n",
    "with open(outfile,'a') as f:\n",
    "    for s in transcript:\n",
    "        f.write(s+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
